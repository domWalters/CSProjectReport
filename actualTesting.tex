\section{Testing}

In order to obtain results that I can evaluate from my algorithm, I am running it with different percentile spreads and 10 times with each of them. At the end of each run I save the entire population with their payoff per year, and select the best one. \newline

The reason I am opting to test at multiple percentile sizes is due to the massive change in search space that can occur depending on which one you use. For example, taking a percentile size of 1 means that each rule can have a value from [1, 2, .., 99]. There are $\approx$ 130 rules; they can be \textless or \textgreater (2 options), and on or off (2 options). Therefore, the corresponding search space size will have an upper bound of $2 \cdot 2 \cdot 99^{130} > 10^{260}$, which is monumental. \newline

Taking a percentile size of 5 instead results in a search space with an upper bound of $2 \cdot 2 \cdot19^{130} > 10^{166}$, a number that is still monumental but far smaller than the first case. I would like to test whether this change in search space size impacts the efficacy of the algorithm, which I believe will be the case. \newline

The percentile sizes that I have chosen are:
\begin{itemize}
    \item 1s - All data and rules are from the set of percentiles \{1, 2, 3, ..., 99\}.
    \item 2s - All data and rules are from the set of percentiles \{2, 4, 6, ..., 98\}.
    \item 4s - All data and rules are from the set of percentiles \{4, 8, 12, ..., 96\}.
    \item 5s - All data and rules are from the set of percentiles \{5, 10, 15, ..., 95\}.
    \item 10s - All data and rules are from the set of percentiles \{10, 20, 30, ..., 90\}.
\end{itemize}

I'll begin by verbosely stating my results along with the best screener in each case, and then going in depth into what the individual screeners represent, whether they make sense in context, and what I believe this means for my algorithm as a whole. I'll summarise with a list of what I think would be good changes.

\subsection{Percentile Gap 1}
Across the ten runs, gap size 1 averaged a 2.15\% payoff per year. The best screen with a payoff of 4.83\% was:

\begin{itemize}
    \item (adjweightedavebasicdilutedsharesos, Gt, 40), \newline (ebitlesscapextointerestex, Lt, 56), (ebitqoqgrowth, Gt, 26), \newline (evtonopat, Lt, 47), (investedcapital, Lt, 42), (sgaextorevenue, Lt , 49), \newline (totalequityandnoncontrollinginterests, Lt, 47)
\end{itemize}

Now taking each of these fields into context:
\begin{itemize}
    \item (adjweightedavebasicdilutedsharesos, Gt, 40) - This suggests that the screener is selecting companies that have high variance in the number of available stocks within certain time frames. It's unclear as to why this is a benefit.
    \item (ebitlesscapextointerestex, Lt, 56) - Preference for this field to be low could mean one of two things:
    \begin{itemize}
        \item The algorithm prefers high CapEx compared to Earnings before Interest, and Tax. This would indicate a preference to businesses that reinvest a lot of their earnings.
        \item The algorithm prefers large interest expenses. This would indicate a preference to businesses that have a comparatively large amount of debt.
    \end{itemize}
    \item (ebitqoqgrowth, Gt, 26) - The algorithm is biasing to companies above a certain earnings growth.
    \item (evtonopat, Lt, 47) - The algorithm is biasing to companies that earn reasonable amounts compared to their enterprise valuation. In other words, the company is not overvalued.
    \item (investedcapital, Lt, 42) - The algorithm is biasing to companies with lower levels of investment from share/debt holders.
    \item (sgaextorevenue, Lt , 49) - This is a measure of profitability which prefers to be low, and the algorithm also prefers it to be low.
    \item (totalequityandnoncontrollinginterests, Lt, 47) - I am unsure as to the definition of this field.
\end{itemize}

Overall, 5 out of these 7 fields seem very reasonable. The other two I am unsure as to their meaning.

\subsection{Percentile Gap 2}
Across the ten runs, gap size 2 averaged a 1.84\% payoff per year. The best screen with a payoff of 3.03\% was:

\begin{itemize}
    \item (bookvaluepershare, Lt, 66), (debttonopat, Gt, 50),\newline (adjweightedavebasicdilutedsharesos, Gt, 40), (dividendyield, Lt, 40),\newline (ebit, Gt, 40), (ebitda, Gt, 58), (totalliabilities, Lt, 62)
\end{itemize}

Now taking each of these fields into context:
\begin{itemize}
    \item (bookvaluepershare, Lt, 66) - The algorithm either prefers smaller companies, or companies with less shares.
    \item (debttonopat, Gt, 50) - The algorithm prefers companies that have a larger amount of debt, comparative to their net operating profit.
    \item (adjweightedavebasicdilutedsharesos, Gt, 40) - This suggests that the screener is selecting companies that have high variance in the number of available stocks within certain time frames. It's unclear as to why this is a benefit. This is also included in the Gap 1 case, and even to the exact same degree.
    \item (dividendyield, Lt, 40) - The algorithm prefers companies that pay less to their shareholders.
    \item (ebit, Gt, 40) - The algorithm prefers companies with higher earnings.
    \item (ebitda, Gt, 58) - Same as above. One of these is likely superfluous.
    \item (totalliabilities, Lt, 62) - The algorithm prefers companies with lower aggregate debt.
\end{itemize}

Overall, 6 out of 7 fields seem reasonable. 1 unknown.

\subsection{Percentile Gap 4}
Across the ten runs, gap size 4 averaged a 1.94\% payoff per year. The best screen with a payoff of 3.5\% was:

\begin{itemize}
    \item (adjweightedavebasicsharesos, Gt, 20), (finleverage, Lt, 12),\newline (marketcap, Lt, 72), (netnonopex, Lt, 32), (pricetorevenue, Lt, 28)
\end{itemize}

Now taking each of these fields into context:
\begin{itemize}
    \item (adjweightedavebasicsharesos, Gt, 20) - Same as for above two, I'm not sure why this is the case.
    \item (finleverage, Lt, 12) - The algorithm prefers companies with low interest payments.
    \item (marketcap, Lt, 72) - The algorithm prefers smaller companies.
    \item (netnonopex, Lt, 32) - The algorithm prefers lower non-business related expenses.
    \item (pricetorevenue, Lt, 28) - The algorithm prefers potentially undervalued businesses.
\end{itemize}

Overall, 4 out of 5 fields seem reasonable. 1 unknown.

\subsection{Percentile Gap 5}
Across the ten runs, gap size 5 averaged a 1.89\% payoff per year. The best screen with a payoff of 3.22\% was:

\begin{itemize}
    \item (croic, Gt, 40), (ebittointerestex, Gt, 40), (evtoebitda, Gt, 40),\newline (evtoocf, Lt, 45), (faturnover, Lt, 45), (marketcap, Lt, 30),\newline (nopatqoqgrowth, Gt, 40), (totalassets, Lt, 55)
\end{itemize}

Now taking each of these fields into context:
\begin{itemize}
    \item (croic, Gt, 40) - The algorithm prefers higher CROIC. The measure CROIC is defined such that higher CROIC is better.
    \item (ebittointerestex, Gt, 40) - The algorithm prefers high earnings compared to expenses.
    \item (evtoebitda, Gt, 40) - The algorithm prefers high valuation compared to earnings. But evtoebitda is normally preferred to be lower.
    \item (evtoocf, Lt, 45) - The algorithm prefers companies that can reinvest in themselves quicker.
    \item (faturnover, Lt, 45) - The algorithm prefers companies that aren't as effective at utilising fixed assets. This is kinda odd.
    \item (marketcap, Lt, 30) - The algorithm prefers smaller companies.
    \item (nopatqoqgrowth, Gt, 40) - The algorithm prefers companies with faster growing net operating profit.
    \item (totalassets, Lt, 55) - The algorithm prefers companies with lower value of fixed assets.
\end{itemize}

This is probably the weirdest screener in this test. It prefers a low asset value, with bad asset utilisation. It prefers high valuation compared to earnings, but low valuation compared to free cash flow.

\subsection{Percentile Gap 10}
Across the ten runs, gap size 10 averaged a 1.74\% payoff per year. The best screen with a payoff of 4.42\% was:

\begin{itemize}
    \item (adjbasiceps, Lt, 10), (adjweightedavedilutedsharesos, Gt, 30), \newline (ltdebtandcapleases, Lt, 30), (sgaextorevenue, Lt, 10),\newline (totalcapital, Gt, 40)
\end{itemize}

Now taking each of these fields into context:
\begin{itemize}
    \item (adjbasiceps, Lt, 10) - The algorithm either prefers lower net income, higher dividends, or high outstanding share count.
    \item (adjweightedavedilutedsharesos, Gt, 30) - Same as for above three. This time this makes sense, as it also wants low adjbascieps which a high value of this stat will cause.
    \item (ltdebtandcapleases, Lt, 30) - The algorithm prefers a lower amount of long term debt.
    \item (sgaextorevenue, Lt, 10) - This is a measure of profitability which prefers to be low, and the algorithm prefers it to be very low.
    \item (totalcapital, Gt, 40) - This could either mean that the algorithm prefers a higher amount of long term debt, or of total shareholder equity. Considering that it is explicitly minimising the former, I assume it must be the latter.
\end{itemize}

This screener seems entirely reasonable.

\subsection{Summary of Findings}

\begin{itemize}
    \item Finer grain percentiles perform better on average.
    \item Large variance in screener quality probably means that initialisation is very important in deciding whether the screeners will be good. This is probably due to the size of the search space, and lack of niching methods in my algorithm.
    \item Screeners are mostly consistent; they consistently like and dislike the same things:
    \begin{itemize}
        \item Smaller businesses are preferred.
        \item High earnings are preferred.
    \end{itemize}
    \item Populations converge completely by the end, this is potentially not great and likely impacts my second point as well.
\end{itemize}

\subsection{Changes to make}

\begin{itemize}
    \item Add some form of niching. Initially my idea is to make sure that each new member doesn't have the exact same used list as any other existing member.
    \item Stop initialising 50\% of the used fields to be true. Instead, initialise 10 on average. This tallies up with what successful screeners generally look like when the algorithm ends.
    \item As a result of the above, remove the ratio system. This is basically an artifact of an early implementation.
    \item Again as a result of the above, delete the recalc\_fields\_used method. We no longer need to unitnitialise bad fields for the purpose of the ratio metric. They will naturally be removed by evolution.
\end{itemize}