\section{Contexualising the screens} \label{contextualising}
Once my system was functional, I set about trying to contextualise to myself exactly what the prospective screens that it was optimising towards meant in a financial context, and whether they were sensible. I immediately ran into an assortment of issues.

\subsection{Invalid Screener Fields} \label{invalidField}
Among the elements of the proposed screeners were entries where the boolean switch was on, but the data in the screener made no sense. \newline

For example, in several runs of the algorithm there were fields that were turned on that had values that were so small that they filtered out less than 5 stocks out of the 5000 that I had at the time. My interpretation of this behaviour is that by optimising the field to be almost non existent, the algorithm is effectively forcing the value of the field to be ignored even though the boolean switch still designates the field as relevant. This should be taken into account when the boolean switches are flipped. \newline

I fixed this by creating a sorted vector of all of the training data samples for each field (at initialisation), and ensuring that any prospective field in the screener filters out at least 0.1\% of the training data. This ensures a baseline level of relevance for each field in the screener.

\subsection{The Empty Screen} \label{emptyScreen}
Further to the above concern, I also noticed that many of the Player objects - when the algorithm terminated - were using ``the empty screen''; the screen where every element is turned off. This is potentially detrimental to the efficacy of the algorithm, as this effectively means that the population is shrinking over time. \newline

To prevent this, I am ensuring that the resultant Player created from the selection-crossover-mutation chain has at least one field turned on. The algorithm simply loops through repeatedly creating more ``children'', until it generates a descendent with at least one field.

\subsection{Badly Formed Screeners}
A concern connected to \ref{invalidField} is that the screener provided by the algorithm would often be badly formed. These badly formed screeners can be split into two categories.

\subsubsection{Long Screeners} \label{longScreeners}
Standard screening strategies used by current stocks traders contain fewer rules than the screeners that my algorithm creates. For example, the current top 5 screening strategies (by return on investment) listed on Stockopedia contain: 6, 5, 4, 5, and 7 rules respectively. \cite{stockopediaScreens} Whereas, my algorithm outputs screens of anywhere from 10-30 rules in a standard run. \newline

As a result of this considerable disparity, I decided to transform the payoff from the stocks trading game by multiplying it by a punishing factor:
\begin{equation}
    new = \frac{old}{max(10, max(15 - len(Screener), len(Screener)))}
\end{equation}

This punishing factor is easier to understand as a graph: \newline
\begin{center}
    \begin{tikzpicture}
        \begin{axis}[
            axis lines=left, 
            xlabel=len(Screener), 
            ylabel=payoff multiplier, 
            ytick={0.02,0.04,0.06,0.08,0.1},
            yticklabels={0.02,0.04,0.06,0.08,0.1},
            xmin=0, xmax=30, 
            ymin=0, ymax=0.1]
            \addplot [domain=0:5, samples=100, color=red]{1 / (15-x)};
            \addplot [domain=5:10, samples=100, color=red]{1 / 10};
            \addplot [domain=10:100, samples=100, color=red]{1 / x};
        \end{axis}
    \end{tikzpicture}
\end{center}

So, screens containing 5 to 10 rules are punished the least, and by a constant amount so that none of these lengths are preferred over the others. Below 5 rules and above 10 rules, screeners are punished in an exponentially decreasing manner based on their length. This allows the algorithm to prefer screening strategies that are similar in structure to human designed strategies, but can still consider smaller/larger strategies.

\subsubsection{Screeners that buy very few stocks}
A useful screening strategy should buy a reasonable number of stocks. Currently, it is possible for my algorithm to produce screening strategies that purchase as little as zero stocks. Looking at the top 5 strategies on Stockopedia again, they are ``buying'': 200, 25, 100, 30, and 47 stocks respectively this quarter. \newline

To minimise the number of strategies purchasing very few stocks, I decided to transform the payoff again by multiplying it by a rewarding factor:

\begin{equation}
    new = old * min(20, len(PurchaseList))
\end{equation}

Again, as a graph: \newline
\begin{center}
    \begin{tikzpicture}
        \begin{axis}[
            axis lines = left,
            xlabel = length of purchase list,
            ylabel = payoff multiplier,
            xmin=0, xmax=100,
            ymin=0, ymax=20,
        ]
        \addplot [domain=0:20, samples=100, color=red]{x};
        \addplot [domain=20:100, samples=100, color=red]{20};
        \end{axis}
    \end{tikzpicture}
\end{center}

So screeners are rewarded until they purchase more than 20 stocks, whence-forth the reward becomes constant. This allows the algorithm to prefer screening strategies that purchase more stocks.

\subsubsection{Summary}
To conclude, I am combining both of the multiplications above into the following expression for the new payoff:
\begin{equation}
    new = old * 
    \frac{min(20, len(PurchaseList))}{max(10, max(15 - len(Screener), len(Screener)))}
\end{equation}

\subsection{Using price to optimise price}
I anticipated early in the project that I would run into an issue where prospective screeners relied on past stock price to predict future stock price. It is widely accepted in the stock trading world that this is not a good practice - past stock trends don't influence future trends. \cite{weakFormEfficiency} \newline

I elected to add a list of ``banned'' fields, so that when the algorithm initially starts, these fields are initialised to be unused and therefore ignored by the algorithm.

\subsection{Overfitting}
A concern that permeates all of the machine learning disciplines is that of overfitting. Usually caused when the algorithm is repeatedly trained on the same data set, the machine no longer simply learns the trends of the data but instead memorises the actual data itself. This damages the predictive power of the algorithm, as it will no longer be able to sufficiently characterise data that it hasn't seen before.

\subsubsection{Resetting on iteration} \label{iterReset}
I have decided that after each iteration, the Players each have their screener reset to a random valid value, whilst the fields used parameter is unaffected. \newline

When fields are removed by the algorithm (e.g. by the methods from Section \ref{invalidField}) the search space changes. This new search space will be larger than the current one, so screeners that have already optimised will miss out on this new area to search unless they are reset.

\subsection{Percentile Vs Raw Floats}
A suggestion made to me by my supervisor, was that my search space was almost certainly too large. By using floating point numbers with huge ranges (some of these ranges being from the negative billions to the positive billions), I am considering a search space that is far too large to optimise over. \newline

He made the suggestion that instead of using complex floating point data, I should simplify the problem to use integer percentiles for each field of training data. By making this change, each field has a much smaller range of values that it can take, and can therefore be searched more effectively. It also lowers the impact of incredibly high or low values.

\subsection{Upper Limits}
A decision I made early on in the design process was to only allow the Screener to consist of lower bounds on each field. Every rule it optimised was of the form ``field \textgreater \, lower\_bound''. I made this decision for simplicity sake in early development. Now that my system is functional, I can add the case for the upper limit, ``field \textless \, upper\_bound''. \newline